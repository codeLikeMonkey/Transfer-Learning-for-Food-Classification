{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, hidden_layers)\n",
    "        self.fc3 = nn.Linear(hidden_layers, output_size)\n",
    "    \n",
    "    def forward(self,data):\n",
    "        x = F.tanh(self.fc1(data))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x,dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(label_id):\n",
    "    vector = np.zeros(101)\n",
    "    vector[label_id] = 1\n",
    "    return vector\n",
    "\n",
    "def onehot2label(vector):\n",
    "    return np.argmax(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(data,batch_size):\n",
    "    batches = []\n",
    "    x = np.arange(len(data) // batch_size + 2)*batch_size\n",
    "#     print(x)\n",
    "    for b,e in zip(x[0:-1],x[1:]):\n",
    "        batches.append(data[b:e])\n",
    "    return batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(minibatch):\n",
    "    label_batch = []\n",
    "    feature_batch_vector = []\n",
    "    for label, feature in minibatch:\n",
    "        label_batch.append(label)\n",
    "        feature_batch_vector.append(feature.squeeze())\n",
    "    return np.array(label_batch), np.array(feature_batch_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_data,net):\n",
    "    correct = 0\n",
    "    for label, feature in test_data:\n",
    "        x = Variable(torch.FloatTensor(feature)).cuda()\n",
    "        output = net(x)\n",
    "        if onehot2label(output.cpu().data.numpy()) == label:\n",
    "            correct += 1\n",
    "    return correct / len(test_data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data_vgg19.pkl\",\"rb\") as f:\n",
    "    training_data = pickle.load(f)\n",
    "    random.shuffle(training_data)\n",
    "    \n",
    "with open(\"test_data_vgg19.pkl\",\"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = chunk(training_data,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep[0]\t training acc: 0.330799\t test acc: 0.338178\n",
      "ep[1]\t training acc: 0.377282\t test acc: 0.372040\n",
      "ep[2]\t training acc: 0.402419\t test acc: 0.384990\n",
      "ep[3]\t training acc: 0.422525\t test acc: 0.392079\n",
      "ep[4]\t training acc: 0.441773\t test acc: 0.399485\n",
      "ep[5]\t training acc: 0.454869\t test acc: 0.402772\n",
      "ep[6]\t training acc: 0.464956\t test acc: 0.404238\n",
      "ep[7]\t training acc: 0.473405\t test acc: 0.405228\n",
      "ep[8]\t training acc: 0.480547\t test acc: 0.407564\n",
      "ep[9]\t training acc: 0.487254\t test acc: 0.407802\n",
      "ep[10]\t training acc: 0.493353\t test acc: 0.409307\n",
      "ep[11]\t training acc: 0.499822\t test acc: 0.409743\n",
      "ep[12]\t training acc: 0.504561\t test acc: 0.410772\n",
      "ep[13]\t training acc: 0.512614\t test acc: 0.411406\n",
      "ep[14]\t training acc: 0.522780\t test acc: 0.418416\n",
      "ep[15]\t training acc: 0.527796\t test acc: 0.418733\n",
      "ep[16]\t training acc: 0.536721\t test acc: 0.421149\n",
      "ep[17]\t training acc: 0.541619\t test acc: 0.424277\n",
      "ep[18]\t training acc: 0.549804\t test acc: 0.425267\n",
      "ep[19]\t training acc: 0.554675\t test acc: 0.429228\n",
      "ep[20]\t training acc: 0.561144\t test acc: 0.428238\n",
      "ep[21]\t training acc: 0.572868\t test acc: 0.436871\n",
      "ep[22]\t training acc: 0.578465\t test acc: 0.437030\n",
      "ep[23]\t training acc: 0.582386\t test acc: 0.436594\n",
      "ep[24]\t training acc: 0.586611\t test acc: 0.435802\n",
      "ep[25]\t training acc: 0.590360\t test acc: 0.437505\n",
      "ep[26]\t training acc: 0.590254\t test acc: 0.434020\n",
      "ep[27]\t training acc: 0.591192\t test acc: 0.432911\n",
      "ep[28]\t training acc: 0.602823\t test acc: 0.438337\n",
      "ep[29]\t training acc: 0.615179\t test acc: 0.443446\n",
      "ep[30]\t training acc: 0.622084\t test acc: 0.446614\n",
      "ep[31]\t training acc: 0.626612\t test acc: 0.444911\n",
      "ep[32]\t training acc: 0.630850\t test acc: 0.446733\n",
      "ep[33]\t training acc: 0.639378\t test acc: 0.450812\n",
      "ep[34]\t training acc: 0.646309\t test acc: 0.454653\n",
      "ep[35]\t training acc: 0.649016\t test acc: 0.452119\n",
      "ep[36]\t training acc: 0.653438\t test acc: 0.454416\n",
      "ep[37]\t training acc: 0.660422\t test acc: 0.457426\n",
      "ep[38]\t training acc: 0.661663\t test acc: 0.456356\n",
      "ep[39]\t training acc: 0.667604\t test acc: 0.459406\n",
      "ep[40]\t training acc: 0.666574\t test acc: 0.454337\n",
      "ep[41]\t training acc: 0.669188\t test acc: 0.457069\n",
      "ep[42]\t training acc: 0.674390\t test acc: 0.458614\n",
      "ep[43]\t training acc: 0.675287\t test acc: 0.455248\n",
      "ep[44]\t training acc: 0.678047\t test acc: 0.454614\n",
      "ep[45]\t training acc: 0.684106\t test acc: 0.459881\n",
      "ep[46]\t training acc: 0.686667\t test acc: 0.462139\n",
      "ep[47]\t training acc: 0.687064\t test acc: 0.459406\n",
      "ep[48]\t training acc: 0.690338\t test acc: 0.457941\n",
      "ep[49]\t training acc: 0.695156\t test acc: 0.461267\n",
      "ep[50]\t training acc: 0.695222\t test acc: 0.459644\n",
      "ep[51]\t training acc: 0.698113\t test acc: 0.460277\n",
      "ep[52]\t training acc: 0.700635\t test acc: 0.460752\n",
      "ep[53]\t training acc: 0.703368\t test acc: 0.459683\n",
      "ep[54]\t training acc: 0.706035\t test acc: 0.460198\n",
      "ep[55]\t training acc: 0.705837\t test acc: 0.460079\n",
      "ep[56]\t training acc: 0.712200\t test acc: 0.462733\n",
      "ep[57]\t training acc: 0.714008\t test acc: 0.465030\n",
      "ep[58]\t training acc: 0.719395\t test acc: 0.464990\n",
      "ep[59]\t training acc: 0.722695\t test acc: 0.469347\n",
      "ep[60]\t training acc: 0.724662\t test acc: 0.467564\n",
      "ep[61]\t training acc: 0.727976\t test acc: 0.473347\n",
      "ep[62]\t training acc: 0.729428\t test acc: 0.472554\n",
      "ep[63]\t training acc: 0.731395\t test acc: 0.474337\n",
      "ep[64]\t training acc: 0.731620\t test acc: 0.470772\n",
      "ep[65]\t training acc: 0.736399\t test acc: 0.472752\n",
      "ep[66]\t training acc: 0.739052\t test acc: 0.476396\n",
      "ep[67]\t training acc: 0.742379\t test acc: 0.477228\n",
      "ep[68]\t training acc: 0.743290\t test acc: 0.474535\n",
      "ep[69]\t training acc: 0.745772\t test acc: 0.473941\n",
      "ep[70]\t training acc: 0.747937\t test acc: 0.474376\n",
      "ep[71]\t training acc: 0.749997\t test acc: 0.474535\n",
      "ep[72]\t training acc: 0.748809\t test acc: 0.473505\n",
      "ep[73]\t training acc: 0.752809\t test acc: 0.477069\n",
      "ep[74]\t training acc: 0.754168\t test acc: 0.476713\n",
      "ep[75]\t training acc: 0.751449\t test acc: 0.470218\n",
      "ep[76]\t training acc: 0.755555\t test acc: 0.472277\n",
      "ep[77]\t training acc: 0.757073\t test acc: 0.476832\n",
      "ep[78]\t training acc: 0.757311\t test acc: 0.475802\n",
      "ep[79]\t training acc: 0.758485\t test acc: 0.476079\n",
      "ep[80]\t training acc: 0.759872\t test acc: 0.479208\n",
      "ep[81]\t training acc: 0.760769\t test acc: 0.474653\n",
      "ep[82]\t training acc: 0.760730\t test acc: 0.473465\n",
      "ep[83]\t training acc: 0.761047\t test acc: 0.476356\n",
      "ep[84]\t training acc: 0.762974\t test acc: 0.475723\n",
      "ep[85]\t training acc: 0.763423\t test acc: 0.476119\n",
      "ep[86]\t training acc: 0.763991\t test acc: 0.480436\n",
      "ep[87]\t training acc: 0.764862\t test acc: 0.476752\n",
      "ep[88]\t training acc: 0.765416\t test acc: 0.477505\n",
      "ep[89]\t training acc: 0.765047\t test acc: 0.473069\n",
      "ep[90]\t training acc: 0.766895\t test acc: 0.479089\n",
      "ep[91]\t training acc: 0.767819\t test acc: 0.482178\n",
      "ep[92]\t training acc: 0.768176\t test acc: 0.477703\n",
      "ep[93]\t training acc: 0.768466\t test acc: 0.477267\n",
      "ep[94]\t training acc: 0.769377\t test acc: 0.478178\n",
      "ep[95]\t training acc: 0.769958\t test acc: 0.479564\n",
      "ep[96]\t training acc: 0.768255\t test acc: 0.473861\n",
      "ep[97]\t training acc: 0.769311\t test acc: 0.474812\n",
      "ep[98]\t training acc: 0.771503\t test acc: 0.481069\n",
      "ep[99]\t training acc: 0.771925\t test acc: 0.479168\n",
      "ep[100]\t training acc: 0.772044\t test acc: 0.476911\n",
      "ep[101]\t training acc: 0.772215\t test acc: 0.478495\n",
      "ep[102]\t training acc: 0.772809\t test acc: 0.476158\n",
      "ep[103]\t training acc: 0.773443\t test acc: 0.477703\n",
      "ep[104]\t training acc: 0.773588\t test acc: 0.478653\n",
      "ep[105]\t training acc: 0.774486\t test acc: 0.478020\n",
      "ep[106]\t training acc: 0.775186\t test acc: 0.482020\n",
      "ep[107]\t training acc: 0.775529\t test acc: 0.477545\n",
      "ep[108]\t training acc: 0.775529\t test acc: 0.475168\n",
      "ep[109]\t training acc: 0.776348\t test acc: 0.480119\n",
      "ep[110]\t training acc: 0.776889\t test acc: 0.480356\n",
      "ep[111]\t training acc: 0.776691\t test acc: 0.476158\n",
      "ep[112]\t training acc: 0.777364\t test acc: 0.477822\n",
      "ep[113]\t training acc: 0.777971\t test acc: 0.476554\n",
      "ep[114]\t training acc: 0.778764\t test acc: 0.478970\n",
      "ep[115]\t training acc: 0.778948\t test acc: 0.477901\n",
      "ep[116]\t training acc: 0.779358\t test acc: 0.474733\n",
      "ep[117]\t training acc: 0.780255\t test acc: 0.480158\n",
      "ep[118]\t training acc: 0.781140\t test acc: 0.477426\n",
      "ep[119]\t training acc: 0.781576\t test acc: 0.479168\n",
      "ep[120]\t training acc: 0.782090\t test acc: 0.476832\n",
      "ep[121]\t training acc: 0.782460\t test acc: 0.476119\n",
      "ep[122]\t training acc: 0.783807\t test acc: 0.478495\n",
      "ep[123]\t training acc: 0.784638\t test acc: 0.478614\n",
      "ep[124]\t training acc: 0.784902\t test acc: 0.476634\n",
      "ep[125]\t training acc: 0.785853\t test acc: 0.478851\n"
     ]
    }
   ],
   "source": [
    "net = Classifier(4096,2048,101)\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate )\n",
    "loss = nn.CrossEntropyLoss()\n",
    "net.train()\n",
    "net.cuda()\n",
    "max_epoch = 150\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "for ep in range(max_epoch):\n",
    "    err_sum = 0 \n",
    "    for minibatch in training_batches:\n",
    "        feature_label, feature_vector = encode_batch(minibatch)\n",
    "        feature_tensor = Variable(torch.FloatTensor(feature_vector)).cuda()\n",
    "        feature_label = Variable(torch.LongTensor(feature_label)).cuda()\n",
    "        output = net(feature_tensor)\n",
    "        error = loss(output, feature_label)\n",
    "        err_sum += error\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "    avg_err = err_sum.cpu().data.numpy()/ len(minibatch)\n",
    "    test_acc.append(validate(test_data,net))\n",
    "    train_acc.append(validate(training_data,net))\n",
    "    print(\"ep[%d]\\t training acc: %f\\t test acc: %f\"%(ep, train_acc[-1], test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(train_acc))\n",
    "plt.plot(np.array(test_acc))\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)  + len(test_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
